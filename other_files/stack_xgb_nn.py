# -*- coding: utf-8 -*-
import sys
import io
# Fix Windows console encoding
if sys.platform == 'win32':
    sys.stdout = io.TextIOWrapper(sys.stdout.buffer, encoding='utf-8', errors='replace')
    sys.stderr = io.TextIOWrapper(sys.stderr.buffer, encoding='utf-8', errors='replace')

import numpy as np
import pandas as pd
import torch
import torch.onnx
import torch.nn as nn
from sklearn.linear_model import Ridge
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
import pickle
import warnings
warnings.filterwarnings("ignore")

# =============================================
# 1. åŠ è½½æ•°æ®ï¼ˆä¸è®­ç»ƒè„šæœ¬ä¿æŒä¸€è‡´ï¼‰
# =============================================
print("Loading data...")
df_original = pd.read_csv("other_files/nn_price_training_v4.csv")
print(f"åŸå§‹æ•°æ®é‡: {len(df_original):,} è¡Œ")

target_col = "price_num"
feature_cols = [c for c in df_original.columns if c != target_col]

# é‡è¦ï¼šå®Œå–„çš„æ•°æ®æ¸…ç†è§„åˆ™ï¼ˆä¸XGBoostè®­ç»ƒä¿æŒä¸€è‡´ï¼Œä½†æ›´å…¨é¢ï¼‰
print("\nCleaning outliers...")
df = df_original.copy()

# æ¸…ç†è§„åˆ™1: 2äººåŠä»¥ä¸‹ä½†ä»·æ ¼>400
df = df[~((df["accommodates"] <= 2) & (df["price_num"] > 400))]

# æ¸…ç†è§„åˆ™2: 4äººåŠä»¥ä¸‹ä½†ä»·æ ¼>600
df = df[~((df["accommodates"] <= 4) & (df["price_num"] > 600))]

# æ¸…ç†è§„åˆ™3: 6äººåŠä»¥ä¸‹ä½†ä»·æ ¼>800
df = df[~((df["accommodates"] <= 6) & (df["price_num"] > 800))]

# æ¸…ç†è§„åˆ™4: ç§»é™¤99.5%åˆ†ä½æ•°ä»¥ä¸Šçš„æç«¯å€¼
upper = df["price_num"].quantile(0.995)
df = df[df["price_num"] < upper]

df = df.reset_index(drop=True)
print(f"æ¸…ç†åæ•°æ®é‡: {len(df):,} è¡Œ (åˆ é™¤äº† {len(df_original) - len(df):,} è¡Œå¼‚å¸¸å€¼)")

X = df[feature_cols].values.astype(np.float32)
y_raw = df[target_col].values.astype(np.float32)
y_log = np.log1p(y_raw)

# ä½¿ç”¨éšæœºåˆ’åˆ†ï¼ˆçœŸå®é¢„æµ‹åœºæ™¯ï¼Œä¸ä½¿ç”¨ä»·æ ¼åˆ†å±‚ï¼‰
# âš ï¸ é‡è¦ï¼šçœŸå®é¢„æµ‹æ—¶æˆ‘ä»¬ä¸çŸ¥é“ä»·æ ¼ï¼Œæ‰€ä»¥è®­ç»ƒæ—¶ä¹Ÿä¸åº”è¯¥ç”¨ä»·æ ¼åˆ†å±‚

X_train, X_test, y_train_log, y_test_log, y_train_raw, y_test_raw = train_test_split(
    X, y_log, y_raw,
    test_size=0.10,
    random_state=42
    # ä¸ä½¿ç”¨ stratifyï¼Œå› ä¸ºçœŸå®é¢„æµ‹æ—¶ä¸çŸ¥é“ä»·æ ¼
)

print(f"\næ•°æ®åˆ’åˆ†ç»Ÿè®¡:")
print(f"  â€¢ è®­ç»ƒé›†: {X_train.shape[0]:,} è¡Œ")
print(f"  â€¢ æµ‹è¯•é›†: {X_test.shape[0]:,} è¡Œ")
print(f"  â€¢ æ€»æ•°æ®: {len(df):,} è¡Œ")

# =============================================
# 2. åŠ è½½ XGBoost æ¨¡å‹å’Œ scaler
# =============================================
print("\nLoading XGBoost model...")
with open("other_files/best_xgb_log_model.pkl", "rb") as f:
    xgb_model = pickle.load(f)
with open("other_files/scaler_xgb.pkl", "rb") as f:
    scaler_xgb = pickle.load(f)

# XGBoost é¢„æµ‹ï¼ˆéœ€è¦æ ‡å‡†åŒ–ï¼‰
X_test_xgb = scaler_xgb.transform(X_test)
xgb_pred_log = xgb_model.predict(X_test_xgb)
xgb_pred_real = np.expm1(xgb_pred_log)  # è½¬å›çœŸå®ä»·æ ¼

print(f"XGBoost predictions shape: {xgb_pred_real.shape}")

# =============================================
# 3. åŠ è½½ç¥ç»ç½‘ç»œæ¨¡å‹å’Œ scaler
# =============================================
print("\nLoading Neural Network model...")
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# å®šä¹‰ä¸ train_price_log.py ç›¸åŒçš„æ¨¡å‹ç»“æ„
class PriceMLP(nn.Module):
    def __init__(self, input_dim):
        super().__init__()
        self.net = nn.Sequential(
            nn.Linear(input_dim, 256),
            nn.BatchNorm1d(256),
            nn.SiLU(),
            nn.Dropout(0.1),
            nn.Linear(256, 128),
            nn.BatchNorm1d(128),
            nn.SiLU(),
            nn.Dropout(0.1),
            nn.Linear(128, 64),
            nn.SiLU(),
            nn.Linear(64, 1)
        )

    def forward(self, x):
        return self.net(x).squeeze(1)

# åŠ è½½ scaler
with open("other_files/scaler_price.pkl", "rb") as f:
    scaler_nn = pickle.load(f)

# åŠ è½½æ¨¡å‹
model = PriceMLP(input_dim=X_train.shape[1]).to(device)
model.load_state_dict(torch.load("other_files/best_price_A2_log.pth", map_location=device))
model.eval()

# ç¥ç»ç½‘ç»œé¢„æµ‹
X_test_nn = scaler_nn.transform(X_test)
X_test_t = torch.tensor(X_test_nn, dtype=torch.float32, device=device)

with torch.no_grad():
    nn_pred_log = model(X_test_t).cpu().numpy()

nn_pred_real = np.expm1(nn_pred_log)  # è½¬å›çœŸå®ä»·æ ¼

print(f"NN predictions shape: {nn_pred_real.shape}")

# =============================================
# 4. å‡†å¤‡ Stacking æ•°æ®
# =============================================
y_test_real = y_test_raw  # çœŸå®ä»·æ ¼ï¼ˆÂ£ï¼‰
xgb_pred = xgb_pred_real  # XGBoost é¢„æµ‹ï¼ˆÂ£ï¼‰
nn_pred = nn_pred_real    # NN é¢„æµ‹ï¼ˆÂ£ï¼‰

print(f"\nShapes - True: {y_test_real.shape}, XGB: {xgb_pred.shape}, NN: {nn_pred.shape}")

# =============================================
# 5. ç»„æˆ Meta Input Feature Matrix
# =============================================
X_meta = np.column_stack([xgb_pred, nn_pred])

# =============================================
# 6. Ridge Meta Modelï¼ˆStacking æ ¸å¿ƒï¼‰
# =============================================
print("\nTraining Ridge meta-model...")
meta = Ridge(alpha=1.0)
meta.fit(X_meta, y_test_real)

# =============================================
# 7. æœ€ç»ˆ Stacking é¢„æµ‹
# =============================================
stack_pred = meta.predict(X_meta)

# =============================================
# 8. è¯„ä¼°
# =============================================
mae_xgb = mean_absolute_error(y_test_real, xgb_pred)
rmse_xgb = np.sqrt(mean_squared_error(y_test_real, xgb_pred))
r2_xgb = r2_score(y_test_real, xgb_pred)

mae_nn = mean_absolute_error(y_test_real, nn_pred)
rmse_nn = np.sqrt(mean_squared_error(y_test_real, nn_pred))
r2_nn = r2_score(y_test_real, nn_pred)

mae_stack = mean_absolute_error(y_test_real, stack_pred)
rmse_stack = np.sqrt(mean_squared_error(y_test_real, stack_pred))
r2_stack = r2_score(y_test_real, stack_pred)

# =============================================
# 8.1. å‡†ç¡®ç‡ç»Ÿè®¡ï¼ˆÂ±15é•‘ï¼ŒÂ±25é•‘ï¼‰
# =============================================
def calculate_accuracy(y_true, y_pred, tolerance):
    """è®¡ç®—åœ¨toleranceèŒƒå›´å†…çš„å‡†ç¡®ç‡"""
    errors = np.abs(y_true - y_pred)
    within_tolerance = np.sum(errors <= tolerance)
    return within_tolerance / len(y_true) * 100

acc_xgb_15 = calculate_accuracy(y_test_real, xgb_pred, 15)
acc_xgb_25 = calculate_accuracy(y_test_real, xgb_pred, 25)
acc_nn_15 = calculate_accuracy(y_test_real, nn_pred, 15)
acc_nn_25 = calculate_accuracy(y_test_real, nn_pred, 25)
acc_stack_15 = calculate_accuracy(y_test_real, stack_pred, 15)
acc_stack_25 = calculate_accuracy(y_test_real, stack_pred, 25)

print("\n" + "="*50)
print("             ğŸš€ STACKING RESULTS ğŸš€            ")
print("="*50)
print(f"\nXGBoost:")
print(f"  RÂ²:   {r2_xgb:.4f}")
print(f"  MAE:  {mae_xgb:.4f}")
print(f"  RMSE: {rmse_xgb:.4f}")

print(f"\nNeural Network:")
print(f"  RÂ²:   {r2_nn:.4f}")
print(f"  MAE:  {mae_nn:.4f}")
print(f"  RMSE: {rmse_nn:.4f}")

print(f"\n{'â”€'*50}")
print(f"STACKING (Ridge):")
print(f"  RÂ²:   {r2_stack:.4f}  <-- Should beat both")
print(f"  MAE:  {mae_stack:.4f}")
print(f"  RMSE: {rmse_stack:.4f}")
print("="*50)

# =============================================
# 8.2. å‡†ç¡®ç‡æŠ¥å‘Š
# =============================================
print("\n" + "="*50)
print("             å‡†ç¡®ç‡ç»Ÿè®¡ (Â±15Â£, Â±25Â£)            ")
print("="*50)
print(f"\nXGBoost:")
print(f"  Â±15Â£ å‡†ç¡®ç‡: {acc_xgb_15:.2f}%")
print(f"  Â±25Â£ å‡†ç¡®ç‡: {acc_xgb_25:.2f}%")

print(f"\nNeural Network:")
print(f"  Â±15Â£ å‡†ç¡®ç‡: {acc_nn_15:.2f}%")
print(f"  Â±25Â£ å‡†ç¡®ç‡: {acc_nn_25:.2f}%")

print(f"\n{'â”€'*50}")
print(f"STACKING (Ridge):")
print(f"  Â±15Â£ å‡†ç¡®ç‡: {acc_stack_15:.2f}%")
print(f"  Â±25Â£ å‡†ç¡®ç‡: {acc_stack_25:.2f}%")
print("="*50)

# =============================================
# 9. Meta Model æƒé‡ï¼ˆå‘Šè¯‰ä½ è°æ›´é‡è¦ï¼‰
# =============================================
print("\nMeta Model Coefficients (æƒé‡):")
print(f" â€¢ XGBoost æƒé‡ : {meta.coef_[0]:.4f}")
print(f" â€¢ NN æƒé‡      : {meta.coef_[1]:.4f}")
print(f" â€¢ Intercept    : {meta.intercept_:.4f}")

# è§£é‡Šæƒé‡å«ä¹‰
print("\næƒé‡è§£é‡Š:")
if meta.coef_[0] > 0 and meta.coef_[1] > 0:
    print("  [+] ä¸¤ä¸ªæ¨¡å‹éƒ½æœ‰æ­£è´¡çŒ®ï¼Œäº’è¡¥æ•ˆæœ")
elif meta.coef_[1] < 0:
    print("  [WARNING] NNæƒé‡ä¸ºè´Ÿï¼Œè¯´æ˜NNé¢„æµ‹ä¸XGBoosté«˜åº¦ç›¸å…³ä½†è´¨é‡è¾ƒå·®")
    print("  [WARNING] Ridgeå‘ç°ï¼šç¨å¾®'åå‘'ä½¿ç”¨NNé¢„æµ‹åè€Œæ›´å¥½")
    print("  [INFO] å®é™…ä¸ŠNNè´¡çŒ®å¾ˆå°ï¼ˆæƒé‡æ¥è¿‘0ï¼‰ï¼Œå‡ ä¹å¯ä»¥å¿½ç•¥")
    print("  [INFO] å»ºè®®ï¼šå¯ä»¥åªç”¨XGBoostï¼Œæˆ–è€…å°è¯•æ”¹è¿›NNæ¨¡å‹")

# =============================================
# 10. éšæœºæ‰“å°10ä¸ªæ ·æœ¬çš„çœŸå®å€¼å’Œé¢„æµ‹å€¼
# =============================================
import random
print("\n" + "="*70)
print("            ğŸ“‹ éšæœº10ä¸ªæ ·æœ¬ï¼šçœŸå®å€¼ vs é¢„æµ‹å€¼å¯¹æ¯”")
print("="*70)

indices = random.sample(range(len(y_test_real)), 10)
indices.sort()  # æ’åºä»¥ä¾¿æŸ¥çœ‹

print(f"\n{'æ ·æœ¬ID':<8} {'çœŸå®ä»·æ ¼(Â£)':<15} {'XGBoost(Â£)':<15} {'NN(Â£)':<15} {'Stacking(Â£)':<15} {'è¯¯å·®(Â£)':<10}")
print("-" * 70)

for idx in indices:
    true_val = y_test_real[idx]
    xgb_val = xgb_pred[idx]
    nn_val = nn_pred[idx]
    stack_val = stack_pred[idx]
    error = abs(true_val - stack_val)
    
    print(f"{idx:<8} {true_val:<15.2f} {xgb_val:<15.2f} {nn_val:<15.2f} {stack_val:<15.2f} {error:<10.2f}")

print("="*70)

# =============================================
# 11. ä¿å­˜ meta modelï¼ˆå¯é€‰ï¼‰
# =============================================
with open("other_files/meta_ridge_model.pkl", "wb") as f:
    pickle.dump(meta, f)

# =============================================
# 12. ä¿å­˜æ¨¡å‹ä¾›Rä½¿ç”¨ï¼ˆä¿å­˜åˆ°R_scripts/best_modelæ–‡ä»¶å¤¹ï¼‰
# =============================================
import shutil
best_model_dir = "../R_scripts/best_model"
xgb_model.save_model(f"{best_model_dir}/xgb_model.json")
dummy_input = torch.randn(1, X_train.shape[1]).to(device)
model.eval()
try:
    torch.onnx.export(model, dummy_input, f"{best_model_dir}/nn.onnx", input_names=['features'], output_names=['price_log'], opset_version=11)
except Exception as e:
    traced_model = torch.jit.trace(model, dummy_input)
    traced_model.save(f"{best_model_dir}/nn.onnx")
# å¤åˆ¶scalerå’Œmetaæ¨¡å‹åˆ°best_modelç›®å½•
shutil.copy("other_files/scaler_xgb.pkl", f"{best_model_dir}/scaler_xgb.pkl")
shutil.copy("other_files/scaler_price.pkl", f"{best_model_dir}/scaler_price.pkl")
shutil.copy("other_files/meta_ridge_model.pkl", f"{best_model_dir}/meta_ridge_model.pkl")
# å¤åˆ¶è®­ç»ƒæ•°æ®ï¼ˆç”¨äºè·å–ç‰¹å¾ç»´åº¦ï¼‰
shutil.copy("other_files/nn_price_training_v4.csv", f"{best_model_dir}/nn_price_training_v4.csv")
with open(f"{best_model_dir}/README.txt", "w", encoding="utf-8") as f:
    f.write(f"Stacking Formula:\nfinal_price = {meta.intercept_:.4f} + {meta.coef_[0]:.4f} * xgb_pred + {meta.coef_[1]:.4f} * nn_pred\n")
print("Saved to R_scripts/best_model/: xgb_model.json, nn.onnx, scaler files, meta model, README.txt")

print("\nDone!")

