# -*- coding: utf-8 -*-
import sys
import io
# Fix Windows console encoding
if sys.platform == 'win32':
    sys.stdout = io.TextIOWrapper(sys.stdout.buffer, encoding='utf-8', errors='replace')
    sys.stderr = io.TextIOWrapper(sys.stderr.buffer, encoding='utf-8', errors='replace')

import numpy as np
import pandas as pd
import torch
import torch.nn as nn
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from numpy.linalg import norm
import pickle
import warnings
warnings.filterwarnings("ignore")

# =============================================
# KNN è¾…åŠ©å‡½æ•°ï¼ˆåŸºäº Autoencoder embeddingï¼Œk=10ï¼‰
# =============================================
def get_knn_prediction(target_embedding, train_embeddings, train_prices, k=10):
    """
    åŸºäº embedding æ‰¾ KNNï¼Œè¿”å›åŠ æƒå¹³å‡ä»·æ ¼
    k=10: åªæ‰¾10ä¸ªæœ€ç›¸ä¼¼çš„æˆ¿æº
    """
    dists = norm(train_embeddings - target_embedding, axis=1)
    topk_idx = np.argsort(dists)[:k]
    
    # åŠ æƒå¹³å‡ï¼ˆè·ç¦»è¶Šè¿‘æƒé‡è¶Šå¤§ï¼‰
    eps = 1e-6
    weights = 1.0 / (dists[topk_idx] + eps)
    weighted_price = np.sum(weights * train_prices[topk_idx]) / np.sum(weights)
    
    return weighted_price

# =============================================
# 1. åŠ è½½æ•°æ®ï¼ˆä¸è®­ç»ƒè„šæœ¬ä¿æŒä¸€è‡´ï¼‰
# =============================================
print("="*70)
print("ğŸš€ XGBoost + NN + KNN (k=10) Stacking with Linear Regression")
print("="*70)

print("\nLoading data...")
df_original = pd.read_csv("nn_price_training_v4.csv")
print(f"åŸå§‹æ•°æ®é‡: {len(df_original):,} è¡Œ")

target_col = "price_num"
feature_cols = [c for c in df_original.columns if c != target_col]

# é‡è¦ï¼šå®Œå–„çš„æ•°æ®æ¸…ç†è§„åˆ™
print("\nCleaning outliers...")
df = df_original.copy()

# æ¸…ç†è§„åˆ™1: 2äººåŠä»¥ä¸‹ä½†ä»·æ ¼>400
df = df[~((df["accommodates"] <= 2) & (df["price_num"] > 400))]

# æ¸…ç†è§„åˆ™2: 4äººåŠä»¥ä¸‹ä½†ä»·æ ¼>600
df = df[~((df["accommodates"] <= 4) & (df["price_num"] > 600))]

# æ¸…ç†è§„åˆ™3: 6äººåŠä»¥ä¸‹ä½†ä»·æ ¼>800
df = df[~((df["accommodates"] <= 6) & (df["price_num"] > 800))]

# æ¸…ç†è§„åˆ™4: ç§»é™¤99.5%åˆ†ä½æ•°ä»¥ä¸Šçš„æç«¯å€¼
upper = df["price_num"].quantile(0.995)
df = df[df["price_num"] < upper]

df = df.reset_index(drop=True)
print(f"æ¸…ç†åæ•°æ®é‡: {len(df):,} è¡Œ (åˆ é™¤äº† {len(df_original) - len(df):,} è¡Œå¼‚å¸¸å€¼)")

X = df[feature_cols].values.astype(np.float32)
y_raw = df[target_col].values.astype(np.float32)
y_log = np.log1p(y_raw)

# ä½¿ç”¨éšæœºåˆ’åˆ†ï¼ˆçœŸå®é¢„æµ‹åœºæ™¯ï¼Œä¸ä½¿ç”¨ä»·æ ¼åˆ†å±‚ï¼‰
# âš ï¸ é‡è¦ï¼šçœŸå®é¢„æµ‹æ—¶æˆ‘ä»¬ä¸çŸ¥é“ä»·æ ¼ï¼Œæ‰€ä»¥è®­ç»ƒæ—¶ä¹Ÿä¸åº”è¯¥ç”¨ä»·æ ¼åˆ†å±‚

X_train, X_test, y_train_log, y_test_log, y_train_raw, y_test_raw = train_test_split(
    X, y_log, y_raw,
    test_size=0.10,
    random_state=42
    # ä¸ä½¿ç”¨ stratifyï¼Œå› ä¸ºçœŸå®é¢„æµ‹æ—¶ä¸çŸ¥é“ä»·æ ¼
)

print(f"\næ•°æ®åˆ’åˆ†ç»Ÿè®¡:")
print(f"  â€¢ è®­ç»ƒé›†: {X_train.shape[0]:,} è¡Œ")
print(f"  â€¢ æµ‹è¯•é›†: {X_test.shape[0]:,} è¡Œ")
print(f"  â€¢ æ€»æ•°æ®: {len(df):,} è¡Œ")

# =============================================
# 2. åŠ è½½ XGBoost æ¨¡å‹å’Œ scaler
# =============================================
print("\n" + "="*70)
print("Loading XGBoost model...")
with open("best_xgb_log_model.pkl", "rb") as f:
    xgb_model = pickle.load(f)
with open("scaler_xgb.pkl", "rb") as f:
    scaler_xgb = pickle.load(f)

# XGBoost é¢„æµ‹ï¼ˆéœ€è¦æ ‡å‡†åŒ–ï¼‰
X_test_xgb = scaler_xgb.transform(X_test)
xgb_pred_log = xgb_model.predict(X_test_xgb)
xgb_pred_real = np.expm1(xgb_pred_log)  # è½¬å›çœŸå®ä»·æ ¼

print(f"[OK] XGBoost predictions shape: {xgb_pred_real.shape}")

# =============================================
# 3. åŠ è½½ç¥ç»ç½‘ç»œæ¨¡å‹å’Œ scaler
# =============================================
print("\nLoading Neural Network model...")
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(f"   Using device: {device}")

# å®šä¹‰ä¸ train_price_log.py ç›¸åŒçš„æ¨¡å‹ç»“æ„
class PriceMLP(nn.Module):
    def __init__(self, input_dim):
        super().__init__()
        self.net = nn.Sequential(
            nn.Linear(input_dim, 256),
            nn.BatchNorm1d(256),
            nn.SiLU(),
            nn.Dropout(0.1),
            nn.Linear(256, 128),
            nn.BatchNorm1d(128),
            nn.SiLU(),
            nn.Dropout(0.1),
            nn.Linear(128, 64),
            nn.SiLU(),
            nn.Linear(64, 1)
        )

    def forward(self, x):
        return self.net(x).squeeze(1)

# åŠ è½½ scaler
with open("scaler_price.pkl", "rb") as f:
    scaler_nn = pickle.load(f)

# åŠ è½½æ¨¡å‹
model = PriceMLP(input_dim=X_train.shape[1]).to(device)
model.load_state_dict(torch.load("best_price_A2_log.pth", map_location=device))
model.eval()

# ç¥ç»ç½‘ç»œé¢„æµ‹
X_test_nn = scaler_nn.transform(X_test)
X_test_t = torch.tensor(X_test_nn, dtype=torch.float32, device=device)

with torch.no_grad():
    nn_pred_log = model(X_test_t).cpu().numpy()

nn_pred_real = np.expm1(nn_pred_log)  # è½¬å›çœŸå®ä»·æ ¼

print(f"[OK] NN predictions shape: {nn_pred_real.shape}")

# =============================================
# 4. åŠ è½½ Autoencoder æ¨¡å‹å¹¶ç”Ÿæˆ KNN é¢„æµ‹ (k=10)
# =============================================
print("\nLoading Autoencoder model for KNN (k=10)...")
try:
    # å®šä¹‰ Autoencoder ç»“æ„ï¼ˆä¸ autoencoder_knn.py ä¸€è‡´ï¼‰
    class Autoencoder(nn.Module):
        def __init__(self, input_dim, latent_dim=16):
            super().__init__()
            self.encoder = nn.Sequential(
                nn.Linear(input_dim, 128),
                nn.ReLU(),
                nn.Linear(128, 64),
                nn.ReLU(),
                nn.Linear(64, latent_dim)
            )
            self.decoder = nn.Sequential(
                nn.Linear(latent_dim, 64),
                nn.ReLU(),
                nn.Linear(64, 128),
                nn.ReLU(),
                nn.Linear(128, input_dim)
            )
        
        def forward(self, x):
            z = self.encoder(x)
            out = self.decoder(z)
            return out, z
    
    # åŠ è½½ Autoencoder scaler å’Œæ¨¡å‹
    with open("ae_scaler.pkl", "rb") as f:
        scaler_ae = pickle.load(f)
    
    latent_dim = 16
    ae_model = Autoencoder(input_dim=X_train.shape[1], latent_dim=latent_dim).to(device)
    ae_model.load_state_dict(torch.load("autoencoder_model.pth", map_location=device))
    ae_model.eval()
    
    # ä¸ºè®­ç»ƒé›†å’Œæµ‹è¯•é›†ç”Ÿæˆ embeddings
    X_train_ae = scaler_ae.transform(X_train)
    X_test_ae = scaler_ae.transform(X_test)
    
    X_train_ae_t = torch.tensor(X_train_ae, dtype=torch.float32, device=device)
    X_test_ae_t = torch.tensor(X_test_ae, dtype=torch.float32, device=device)
    
    with torch.no_grad():
        _, train_embeddings = ae_model(X_train_ae_t)
        train_embeddings = train_embeddings.cpu().numpy()
        
        _, test_embeddings = ae_model(X_test_ae_t)
        test_embeddings = test_embeddings.cpu().numpy()
    
    print(f"   Train embeddings shape: {train_embeddings.shape}")
    print(f"   Test embeddings shape: {test_embeddings.shape}")
    
    # KNN é¢„æµ‹ï¼ˆå¯¹æ¯ä¸ªæµ‹è¯•æ ·æœ¬ï¼Œåœ¨è®­ç»ƒé›†ä¸­æ‰¾æœ€è¿‘çš„ k=10 ä¸ªé‚»å±…ï¼‰
    print("   Computing KNN predictions (k=10)...")
    knn_pred = np.array([
        get_knn_prediction(test_emb, train_embeddings, y_train_raw, k=10)
        for test_emb in test_embeddings
    ])
    
    print(f"[OK] KNN predictions shape: {knn_pred.shape}")
    use_knn = True
    
except FileNotFoundError as e:
    print(f"[WARNING] Autoencoder files not found: {e}")
    print("[WARNING] è·³è¿‡ KNNï¼Œåªä½¿ç”¨ XGBoost + NN stacking")
    use_knn = False
    knn_pred = None
except Exception as e:
    print(f"[WARNING] Error loading Autoencoder: {e}")
    print("[WARNING] è·³è¿‡ KNNï¼Œåªä½¿ç”¨ XGBoost + NN stacking")
    use_knn = False
    knn_pred = None

# =============================================
# 5. å‡†å¤‡ Stacking æ•°æ®
# =============================================
y_test_real = y_test_raw  # çœŸå®ä»·æ ¼ï¼ˆÂ£ï¼‰
xgb_pred = xgb_pred_real  # XGBoost é¢„æµ‹ï¼ˆÂ£ï¼‰
nn_pred = nn_pred_real    # NN é¢„æµ‹ï¼ˆÂ£ï¼‰

print(f"\nPrediction shapes:")
print(f"  â€¢ True prices: {y_test_real.shape}")
print(f"  â€¢ XGBoost: {xgb_pred.shape}")
print(f"  â€¢ NN: {nn_pred.shape}")
if use_knn:
    print(f"  â€¢ KNN (k=10): {knn_pred.shape}")

# =============================================
# 6. ç»„æˆ Meta Input Feature Matrix
# =============================================
if use_knn:
    X_meta = np.column_stack([xgb_pred, nn_pred, knn_pred])
    print(f"\nMeta features: [XGBoost, NN, KNN(k=10)] - shape: {X_meta.shape}")
else:
    X_meta = np.column_stack([xgb_pred, nn_pred])
    print(f"\nMeta features: [XGBoost, NN] - shape: {X_meta.shape}")

# =============================================
# 7. Linear Regression Meta Modelï¼ˆStacking æ ¸å¿ƒï¼‰
# =============================================
print("\n" + "="*70)
print("Training Linear Regression meta-model...")
meta = LinearRegression()
meta.fit(X_meta, y_test_real)

# =============================================
# 8. æœ€ç»ˆ Stacking é¢„æµ‹
# =============================================
stack_pred = meta.predict(X_meta)

# =============================================
# 9. è¯„ä¼°å„ä¸ªæ¨¡å‹
# =============================================
mae_xgb = mean_absolute_error(y_test_real, xgb_pred)
rmse_xgb = np.sqrt(mean_squared_error(y_test_real, xgb_pred))
r2_xgb = r2_score(y_test_real, xgb_pred)

mae_nn = mean_absolute_error(y_test_real, nn_pred)
rmse_nn = np.sqrt(mean_squared_error(y_test_real, nn_pred))
r2_nn = r2_score(y_test_real, nn_pred)

if use_knn:
    mae_knn = mean_absolute_error(y_test_real, knn_pred)
    rmse_knn = np.sqrt(mean_squared_error(y_test_real, knn_pred))
    r2_knn = r2_score(y_test_real, knn_pred)

mae_stack = mean_absolute_error(y_test_real, stack_pred)
rmse_stack = np.sqrt(mean_squared_error(y_test_real, stack_pred))
r2_stack = r2_score(y_test_real, stack_pred)

# =============================================
# 10. å‡†ç¡®ç‡ç»Ÿè®¡ï¼ˆÂ±15Â£ï¼ŒÂ±25Â£ï¼‰
# =============================================
def calculate_accuracy(y_true, y_pred, tolerance):
    """è®¡ç®—åœ¨toleranceèŒƒå›´å†…çš„å‡†ç¡®ç‡"""
    errors = np.abs(y_true - y_pred)
    within_tolerance = np.sum(errors <= tolerance)
    return within_tolerance / len(y_true) * 100

acc_xgb_15 = calculate_accuracy(y_test_real, xgb_pred, 15)
acc_xgb_25 = calculate_accuracy(y_test_real, xgb_pred, 25)
acc_nn_15 = calculate_accuracy(y_test_real, nn_pred, 15)
acc_nn_25 = calculate_accuracy(y_test_real, nn_pred, 25)

if use_knn:
    acc_knn_15 = calculate_accuracy(y_test_real, knn_pred, 15)
    acc_knn_25 = calculate_accuracy(y_test_real, knn_pred, 25)

acc_stack_15 = calculate_accuracy(y_test_real, stack_pred, 15)
acc_stack_25 = calculate_accuracy(y_test_real, stack_pred, 25)

# =============================================
# 11. æ‰“å°ç»“æœ
# =============================================
print("\n" + "="*70)
print("             ğŸš€ STACKING RESULTS ğŸš€            ")
print("="*70)
print(f"\nXGBoost:")
print(f"  RÂ²:   {r2_xgb:.4f}")
print(f"  MAE:  {mae_xgb:.4f}")
print(f"  RMSE: {rmse_xgb:.4f}")
print(f"  Â±15Â£ å‡†ç¡®ç‡: {acc_xgb_15:.2f}%")
print(f"  Â±25Â£ å‡†ç¡®ç‡: {acc_xgb_25:.2f}%")

print(f"\nNeural Network:")
print(f"  RÂ²:   {r2_nn:.4f}")
print(f"  MAE:  {mae_nn:.4f}")
print(f"  RMSE: {rmse_nn:.4f}")
print(f"  Â±15Â£ å‡†ç¡®ç‡: {acc_nn_15:.2f}%")
print(f"  Â±25Â£ å‡†ç¡®ç‡: {acc_nn_25:.2f}%")

if use_knn:
    print(f"\nKNN (Autoencoder-based, k=10):")
    print(f"  RÂ²:   {r2_knn:.4f}")
    print(f"  MAE:  {mae_knn:.4f}")
    print(f"  RMSE: {rmse_knn:.4f}")
    print(f"  Â±15Â£ å‡†ç¡®ç‡: {acc_knn_15:.2f}%")
    print(f"  Â±25Â£ å‡†ç¡®ç‡: {acc_knn_25:.2f}%")

print(f"\n{'â”€'*70}")
print(f"STACKING (Linear Regression):")
print(f"  RÂ²:   {r2_stack:.4f}  <-- Should beat all")
print(f"  MAE:  {mae_stack:.4f}")
print(f"  RMSE: {rmse_stack:.4f}")
print(f"  Â±15Â£ å‡†ç¡®ç‡: {acc_stack_15:.2f}%")
print(f"  Â±25Â£ å‡†ç¡®ç‡: {acc_stack_25:.2f}%")
print("="*70)

# =============================================
# 12. Meta Model æƒé‡ï¼ˆå‘Šè¯‰ä½ è°æ›´é‡è¦ï¼‰
# =============================================
print("\nMeta Model Coefficients (æƒé‡):")
print(f" â€¢ XGBoost æƒé‡ : {meta.coef_[0]:.4f}")
print(f" â€¢ NN æƒé‡      : {meta.coef_[1]:.4f}")
if use_knn:
    print(f" â€¢ KNN æƒé‡     : {meta.coef_[2]:.4f}")
print(f" â€¢ Intercept    : {meta.intercept_:.4f}")

# è§£é‡Šæƒé‡å«ä¹‰
print("\næƒé‡è§£é‡Š:")
if use_knn:
    if all(c > 0 for c in meta.coef_):
        print("  [+] ä¸‰ä¸ªæ¨¡å‹éƒ½æœ‰æ­£è´¡çŒ®ï¼Œäº’è¡¥æ•ˆæœ")
    elif meta.coef_[2] < 0:
        print("  [WARNING] KNNæƒé‡ä¸ºè´Ÿï¼Œè¯´æ˜KNNé¢„æµ‹ä¸å…¶ä»–æ¨¡å‹é«˜åº¦ç›¸å…³ä½†è´¨é‡è¾ƒå·®")
        print("  [INFO] å®é™…ä¸ŠKNNè´¡çŒ®å¯èƒ½è¾ƒå°")
else:
    if all(c > 0 for c in meta.coef_):
        print("  [+] ä¸¤ä¸ªæ¨¡å‹éƒ½æœ‰æ­£è´¡çŒ®ï¼Œäº’è¡¥æ•ˆæœ")
    elif meta.coef_[1] < 0:
        print("  [WARNING] NNæƒé‡ä¸ºè´Ÿï¼Œè¯´æ˜NNé¢„æµ‹ä¸XGBoosté«˜åº¦ç›¸å…³ä½†è´¨é‡è¾ƒå·®")

# =============================================
# 13. éšæœºæ‰“å°10ä¸ªæ ·æœ¬çš„çœŸå®å€¼å’Œé¢„æµ‹å€¼
# =============================================
import random
print("\n" + "="*85)
print("            ğŸ“‹ éšæœº10ä¸ªæ ·æœ¬ï¼šçœŸå®å€¼ vs é¢„æµ‹å€¼å¯¹æ¯”")
print("="*85)

indices = random.sample(range(len(y_test_real)), 10)
indices.sort()  # æ’åºä»¥ä¾¿æŸ¥çœ‹

if use_knn:
    print(f"\n{'æ ·æœ¬ID':<8} {'çœŸå®ä»·æ ¼(Â£)':<15} {'XGBoost(Â£)':<15} {'NN(Â£)':<15} {'KNN(Â£)':<15} {'Stacking(Â£)':<15} {'è¯¯å·®(Â£)':<10}")
    print("-" * 85)
    for idx in indices:
        true_val = y_test_real[idx]
        xgb_val = xgb_pred[idx]
        nn_val = nn_pred[idx]
        knn_val = knn_pred[idx]
        stack_val = stack_pred[idx]
        error = abs(true_val - stack_val)
        
        print(f"{idx:<8} {true_val:<15.2f} {xgb_val:<15.2f} {nn_val:<15.2f} {knn_val:<15.2f} {stack_val:<15.2f} {error:<10.2f}")
else:
    print(f"\n{'æ ·æœ¬ID':<8} {'çœŸå®ä»·æ ¼(Â£)':<15} {'XGBoost(Â£)':<15} {'NN(Â£)':<15} {'Stacking(Â£)':<15} {'è¯¯å·®(Â£)':<10}")
    print("-" * 70)
    for idx in indices:
        true_val = y_test_real[idx]
        xgb_val = xgb_pred[idx]
        nn_val = nn_pred[idx]
        stack_val = stack_pred[idx]
        error = abs(true_val - stack_val)
        
        print(f"{idx:<8} {true_val:<15.2f} {xgb_val:<15.2f} {nn_val:<15.2f} {stack_val:<15.2f} {error:<10.2f}")

print("="*85)

# =============================================
# 14. ä¿å­˜ meta modelï¼ˆå¯é€‰ï¼‰
# =============================================
with open("meta_linear_model.pkl", "wb") as f:
    pickle.dump(meta, f)
print("\nSaved meta model to meta_linear_model.pkl")

print("\nDone!")

